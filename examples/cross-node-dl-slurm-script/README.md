# Multi-Node Deep Learning Training with SLURM and Enroot

This example demonstrates a production-ready SLURM script for scaling deep learning training across multiple nodes and GPUs. The script includes comprehensive monitoring, debugging capabilities, and uses Enroot containers for reproducible environments.

You may adapt this script for your own deep learning workloads by modifying the training command and container image as needed.

This documentation is mostly generated by LLM, please refer to the script files for the implementation.


## Overview

The setup consists of three main components:

1. **SLURM Command Line**: The `sbatch` command with Enroot container configuration, resource allocation, and job parameters
2. **Pre-launch Script** (`train-redpajama-prelaunch.sh`): Main SLURM job script that sets up the distributed training environment and coordinates worker processes
3. **Worker Launcher** (`train-redpajama-launcher.sh`): Worker script executed on each node that handles GPU monitoring and runs the actual training process

## Showcases

### üê≥ Enroot Container Integration
- Uses containerized PyTorch environment for reproducibility
- Automatic container mounting and workspace setup
- Isolated environment with custom working directory

### üìä GPU Monitoring
- Real-time GPU utilization monitoring via `nvidia-smi dmon`
- Power, utilization, memory, and temperature tracking
- Per-node monitoring logs saved to output directory

### üêõ Debugging and Tracing
- Configurable debug mode with detailed bash tracing
- Python fault handler for crash diagnostics
- CUDA launch blocking option for debugging
- NCCL debugging for communication issues

### üöÄ Multi-Node Multi-GPU Launch
- Automatic SLURM environment variable mapping
- Distributed training rank calculation
- Master node and port coordination
- Support for both batch and interactive modes

## Script Components

### 1. SLURM Command Line Configuration

The `sbatch` command is the entry point that configures the entire job environment:

```bash
sbatch --nodes=4 --gpus-per-node=8 --ntasks-per-node=8 --cpus-per-task=28 \
       --account=itscspod --partition=admin \
       --output=/scratch/$USER/out/slurm/slurm-%j.out \
       --container-image=/scratch/$USER/home/containers/pytorch-23.10.sqsh \
       --no-container-mount-home --container-remap-root \
       --container-workdir=/scratch/$USER/py-in-container/lit-gpt \
       --container-mounts=/scratch/$USER/py-in-container/lit-gpt:/scratch/$USER/py-in-container/lit-gpt,/scratch/$USER/data:/scratch/$USER/py-in-container/lit-gpt/data,/scratch/$USER/out:/scratch/$USER/py-in-container/lit-gpt/out \
       ./train-redpajama-prelaunch.sh
```

#### Resource Allocation Parameters

- **`--nodes=4`**: Allocate 4 compute nodes for distributed training
- **`--gpus-per-node=8`**: Request 8 GPUs on each node (typically H100/H800 nodes)
- **`--ntasks-per-node=8`**: Launch 8 MPI tasks per node (1 per GPU for optimal performance)
- **`--cpus-per-task=28`**: Allocate 28 CPU cores per GPU task for data loading and preprocessing

#### Job Management Parameters

- **`--account=itscspod`**: Specify the billing account for resource usage
- **`--partition=admin`**: Target specific node partition (GPU partition in this case)
- **`--output=/scratch/$USER/out/slurm/slurm-%j.out`**: Redirect job output to organized log files

#### Enroot Container Parameters

- **`--container-image=/path/to/pytorch-23.10.sqsh`**: Use pre-built containerized PyTorch environment
- **`--no-container-mount-home`**: Disable automatic home directory mounting for clean environment
- **`--container-remap-root`**: Map container root to user for security
- **`--container-workdir=/scratch/$USER/py-in-container/lit-gpt`**: Set working directory inside container

#### Container Mount Configuration

- **Code Mount**: `/scratch/$USER/py-in-container/lit-gpt:/scratch/$USER/py-in-container/lit-gpt`
  - Maps the training code directory bidirectionally
- **Data Mount**: `/scratch/$USER/data:/scratch/$USER/py-in-container/lit-gpt/data`
  - Maps dataset directory as read-only input
- **Output Mount**: `/scratch/$USER/out:/scratch/$USER/py-in-container/lit-gpt/out`
  - Maps output directory for checkpoints and logs

### 2. Environment Setup (`train-redpajama-prelaunch.sh`)

#### Debug Configuration

```bash
export DEBUG=$(echo "${DEBUG,,}" | grep -q -e '^on\|y\(es\)\?\|t\(rue\)\?\|1$' && echo 1 || echo 0)
export SLURM_INTERACTIVE=$(test -n "$SLURM_PTY_PORT" && echo 1)
```
- **DEBUG**: Enable/disable debug mode with flexible input parsing
- **SLURM_INTERACTIVE**: Auto-detect interactive vs batch mode

#### Python and CUDA Environment

```bash
export PYTHONFAULTHANDLER=${PYTHONFAULTHANDLER:-1}
export CUDA_LAUNCH_BLOCKING=${CUDA_LAUNCH_BLOCKING:0}
export NCCL_DEBUG=${NCCL_DEBUG:-WARN}
export NCCL_DEBUG_SUBSYS=${NCCL_DEBUG_SUBSYS:-INIT,NET}
```
- **PYTHONFAULTHANDLER**: Enable Python crash traceback
- **CUDA_LAUNCH_BLOCKING**: Synchronous CUDA calls for debugging
- **NCCL_DEBUG**: Control NCCL communication logging level

#### Distributed Training Coordination

```bash
export MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | sort | head -n 1)
export MASTER_PORT=54321
export GPT_GLOBAL_SEED="$(python -c 'import numpy; print(numpy.random.randint(2**30-1))')"
```
- **MASTER_ADDR**: First allocated node becomes master
- **MASTER_PORT**: Fixed port for distributed communication
- **GPT_GLOBAL_SEED**: Random seed for reproducible training

#### Timestamping for Long Jobs

```bash
[ -z "$SLURM_INTERACTIVE" ] && (
  bash -c "while sleep 120; do date --iso-8601=sec; done" &
  export GPT_TIMESTAMPING_PID=$!
)
```
- Background process that logs timestamps every 2 minutes
- Helps track job progress in long-running training
- **Note**: Also requires cleanup (`kill $GPT_TIMESTAMPING_PID`) to prevent dangling processes

#### Worker Process Execution and Cleanup

```bash
if [ -z "$SLURM_INTERACTIVE" ]; then
  exec srun --mpi=none ./train-redpajama-launcher.sh
else
  # Interactive mode setup
  exec srun --overlap --nodes=1 --ntasks-per-node=8 [...]
fi

# Kill GPT_TIMESTAMPING
[ -z "$SLURM_INTERACTIVE" ] && kill $GPT_TIMESTAMPING_PID
```

**Cleanup step**: After the main `srun` execution completes (whether successful or failed), the script ensures the background timestamping process is terminated. This prevents the timestamping process from keeping the SLURM job alive indefinitely, which would continue billing charges.

### 3. Worker Launch (`train-redpajama-launcher.sh`)

#### SLURM to Distributed Training Mapping

```bash
export GPT_WORLD_SIZE=$SLURM_STEP_NUM_TASKS
export GPT_NUM_NODES=$SLURM_STEP_NUM_NODES
export GPT_LOCAL_RANK=$SLURM_LOCALID
export GPT_NODE_RANK=$SLURM_NODEID
export GPT_GLOBAL_RANK=$(($SLURM_NODEID * $SLURM_NTASKS_PER_NODE + $SLURM_LOCALID))
export GPT_DEVICES_PER_NODE=${SLURM_GPUS_PER_TASK:-$SLURM_GPUS_PER_NODE}
```
- **WORLD_SIZE**: Total number of processes across all nodes
- **NUM_NODES**: Total allocated nodes
- **LOCAL_RANK**: Process rank within current node (0-7 for 8 GPUs)
- **NODE_RANK**: Current node index (0 to NUM_NODES-1)
- **GLOBAL_RANK**: Unique process ID across all nodes
- **DEVICES_PER_NODE**: GPUs available per node

#### GPU Utilization Monitoring

```bash
if [ -z "$SLURM_INTERACTIVE" ] && [ $SLURM_LOCALID -eq 0 ]; then
  nvidia-smi | grep "NVIDIA H800"
  nvidia-smi dmon --delay 15 --select pumt --options DT \
    --filename $PWD/out/slurm/slurm-$SLURM_JOB_ID.$(hostname).gutil &
  export GPT_NVIDIA_SMI_DMON_PID=$!
fi
```
- Only runs on local rank 0 (one per node) to avoid redundant monitoring
- **pumt**: Power, Utilization, Memory, Temperature metrics
- **DT**: Include date/time in output
- Saves monitoring data to job-specific files

#### Background Process Cleanup

```bash
# Kill NVIDIA_SMI_DMON
[ -z "$SLURM_INTERACTIVE" ] && [ $SLURM_LOCALID -eq 0 ] && kill $GPT_NVIDIA_SMI_DMON_PID
```

**Cleanup step**: After the main `python` execution completes (whether successful or failed), the script ensures the background timestamping process is terminated. This prevents the timestamping process from keeping the SLURM job alive indefinitely, which would continue billing charges.

#### Training Process Launch

```bash
python3 pretrain/redpajama.py --name test --model_name tiny-llama-1.1b \
  --max_steps 131072 --decay_steps 131072 --lr 5e-3 \
  --micro_batch_size 10 --batch_size 2048 --precision bf16-true \
  --parallel_strategy "ddp" $GPT_SLURM_INTERACTIVE \
  --world_size "$GPT_WORLD_SIZE" --num_nodes "$GPT_NUM_NODES" \
  --devices_per_node "$GPT_DEVICES_PER_NODE" --global_rank "$GPT_GLOBAL_RANK" \
  --node_rank "$GPT_NODE_RANK" --local_rank "$GPT_LOCAL_RANK"
```
- Passes all distributed training parameters to the training script
- Uses DistributedDataParallel (DDP) strategy
- Supports resume functionality for fault tolerance

## Usage

### Basic Multi-Node Training

```bash
sbatch --nodes=4 --gpus-per-node=8 --ntasks-per-node=8 --cpus-per-task=28 \
       --account=your_account --partition=gpu_partition \
       --container-image=/path/to/pytorch.sqsh \
       --container-mounts=/scratch/$USER/data:/data,/scratch/$USER/out:/out \
       train-redpajama-prelaunch.sh
```

### Debug Mode

```bash
export DEBUG=1
sbatch [same options as above] train-redpajama-prelaunch.sh
```

### Interactive Development

```bash
srun --nodes=1 --gpus-per-node=8 --ntasks-per-node=8 --mpi=none ./train-redpajama-prelaunch.sh
```

## Customization Guide

### Adapting for Your Use Case

1. **Modify Training Command**: Replace the `python3 pretrain/redpajama.py` command with your training script
2. **Adjust Container**: Update `--container-image` to your custom container
3. **Configure Mounts**: Modify `--container-mounts` for your data paths
4. **Resource Allocation**: Adjust `--nodes`, `--gpus-per-node`, `--cpus-per-task` based on your needs
5. **Account Settings**: Update `--account` and `--partition` for your cluster

### Best Practices for HPC Billing

#### Background Process Management
When adapting this script for your use case, **always ensure proper cleanup of background processes**:

```bash
# When starting background processes
some_monitoring_command &
export MONITORING_PID=$!

# Always kill background processes at script end
kill $MONITORING_PID
```

**Why this matters:**
- Uncleaned background processes keep SLURM jobs running indefinitely
- Results in continuous billing charges even after main work completes
- Wastes shared HPC resources and blocks other users
- Can lead to unexpectedly large compute bills

#### Recommended Cleanup Pattern

```bash
# At the top of your script - set up cleanup trap
cleanup() {
    [ -n "$MONITORING_PID" ] && kill $MONITORING_PID 2>/dev/null || true
    [ -n "$TIMESTAMP_PID" ] && kill $TIMESTAMP_PID 2>/dev/null || true
}
trap cleanup EXIT

# Start background processes
nvidia-smi dmon [...] &
export MONITORING_PID=$!
```

### Environment Variables for Customization

| Variable               | Purpose                        | Default                |
| ---------------------- | ------------------------------ | ---------------------- |
| `DEBUG`                | Enable debug tracing           | `0`                    |
| `CUDA_LAUNCH_BLOCKING` | Synchronous CUDA for debugging | `0`                    |
| `NCCL_DEBUG`           | NCCL logging level             | `WARN`                 |
| `OMP_NUM_THREADS`      | OpenMP threads per process     | `$SLURM_CPUS_PER_TASK` |

## Output Files

- `slurm-{job_id}.out`: Main job output and logs  
- `slurm-{job_id}.{hostname}.gutil`: GPU utilization monitoring per node
- Training checkpoints and logs in configured output directory

## Troubleshooting

### Common Issues

- **NCCL Communication**: Check `NCCL_DEBUG=INFO` for detailed communication logs
- **GPU Memory**: Monitor `.gutil` files for memory usage patterns
- **Container Issues**: Verify container image path and mount permissions
- **Network**: Ensure InfiniBand or high-speed interconnect is available

This script provides a robust foundation for scaling deep learning workloads while maintaining observability and debuggability in production HPC environments.
